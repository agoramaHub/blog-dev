<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-12-03T16:00:06+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Agorama</title><subtitle>Agorama main home page.</subtitle><entry><title type="html">Catinca Malaimare - Artist Residency Program 2020</title><link href="http://localhost:4000/artist-res/2020/11/12/catinca-malaimare.html" rel="alternate" type="text/html" title="Catinca Malaimare - Artist Residency Program 2020" /><published>2020-11-12T13:00:00+00:00</published><updated>2020-11-12T13:00:00+00:00</updated><id>http://localhost:4000/artist-res/2020/11/12/catinca-malaimare</id><content type="html" xml:base="http://localhost:4000/artist-res/2020/11/12/catinca-malaimare.html">&lt;p&gt;&lt;img src=&quot;/img/catinca-1.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“On nights with enough lighting, we are illuminated by light reflected off the Cloud. Where the source of illumination is the dominant spectral line of gold-tinted light scattered at the highest luminance in our faces, we watch with an eye insensitive to other visual pollutants. The nighttime is tinged with yellow biased daylight, and we’re concealed and observant, corner-hidden in the darkest side of the glow dome.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Following on from our collaboration with Offsite Projects artist &lt;a href=&quot;https://www.catincamalaimare.com/&quot;&gt;Catinca Malaimare&lt;/a&gt; is are second artist in residence for our online residency program.&lt;/p&gt;

&lt;p&gt;For her ZIP residency in June Catinca produced ‘Lo Sodium’ a series of performative rituals that dramatised our complex companionship with technology through a series of chorographed interactions and a short story.  Catinca situates herself in and around a large pair of stage lights and invites the viewer to explore these majestic mechanical objects in a theatrical sense, delibreately demanding a discomforting intensity from the viewer and asking us to share the awkward physical presence of these regal (but sadly now redundant) tecnological artefacts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/catinca-2a.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As part of her residency with us at Agorama, Catinca has been experimenting with two TV’s that rebelled against convention, the Philips Discoverer space helmet TV manufactured in the 1980s (see image) under the disguise that they are “worth looking at even when it’s off”. The uncomfortable reality that our machines are dance partners, closer than we are ready to admit, prompted us to work together on the construction of a pair of stage lights where bodies mimic one another.&lt;/p&gt;

&lt;p&gt;Catinca is regularly documenting the process on her &lt;a href=&quot;https://www.instagram.com/catincamalaimare/&quot;&gt;Instagram&lt;/a&gt; and the outputs will be shared publically as part of a group show early next year.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.catincamalaimare.com/&quot;&gt;Catinca Malaimare&lt;/a&gt; is a London based artist and current postgraduate student at The Royal Academy of Arts. Performing alongside anthropomorphised technologies, Malaimare’s choreography manifests our intimate relationship with photographic tools and the screens onto which they project our visages. Recent exhibitions, performances and screenings include 22.200 LEDs, Another Mobile Gallery, Bucharest; LO Sodium, ZIP Agorama X Off Site Project; RA Premiums Interim Show, The Royal Academy of Arts, London (2020); Bad Sights. Bad Sights Ahead, 16 Nicholson Street Gallery, Glasgow; Why Not Pour Video Into Our Eyes?, TUA Geidai, Tokyo; SUNs, STAMPEDE at Royal Academy of Arts; LAND GRAB, Sadie Coles HQ, London (2019); We are all involved in this mess, Enclave Lab; Prefix is post-, Lewisham Art House, London and Pilot Screens, P E R I C L O, Wrexham (2017). Forthcoming exhibitions include group show Dreams Made Flesh, Catinca Tabacaru Gallery, Bucharest (2021). Malaimare is founder and co-curator of Habeas Corpus (We Have A Body), a platform that aims to provide a testing ground for live and media-based works in contemporary performance art.&lt;/p&gt;</content><author><name></name></author><category term="artist-res" /><summary type="html">Catinca Malaimare brings her performative practice to Agorama's artist residency program</summary></entry><entry><title type="html">Shinji Toya, Paintyourfaceaway.net</title><link href="http://localhost:4000/projects/2020/08/30/paint-your-face-away.html" rel="alternate" type="text/html" title="Shinji Toya, Paintyourfaceaway.net" /><published>2020-08-30T11:00:00+00:00</published><updated>2020-08-30T11:00:00+00:00</updated><id>http://localhost:4000/projects/2020/08/30/paint-your-face-away</id><content type="html" xml:base="http://localhost:4000/projects/2020/08/30/paint-your-face-away.html">&lt;p&gt;Shinji Toya’s project, paint your face away, found its  start in life as a desktop application, which Shinji Toya had developed through the use of MAX/MSP visual programming language. Toya utilised this platform for several performative workshops, sharing with audiences and workshop goers the crucial discourse of artificial intelligence face tracking technology. Shinji Toya’s focus for the platform was to subvert the already rooted system of harvesting user’s face data, which is undertaken by most social media and technology platforms, by allowing users to upload an image of themselves and painting their face away, there by obscuring this data from AI harvesters.&lt;/p&gt;

&lt;p&gt;Agorama, was commissioned by Shinji Toya to aid in his platforms transformation from a desktop application to a fully realised web platform. Through this process Agorama, provided consultation services and full stack development work for Toya’s project including the identification of selecting and setting up the correct server setup, integrating face-tracking technology, and web-cam integration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/paintface/paintface-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/paintface/paintface-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Paint Your Face Away was commission by Fotomuseum Winterthur and exhibited as part the museum’s &lt;a href=&quot;https://www.fotomuseum.ch/en/explore/situations/157307&quot;&gt;SITUATIONS/Strike&lt;/a&gt; online exhibition.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://paintyourfaceaway.net/&quot;&gt;paintyourfaceaway.net/&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Full stack development support and consultation for Shinji Toya latest interactive art project &quot;Paint Your Face Away&quot;. Project was commission by Fotomuseum Winterthur and exhibited as part the museum's SITUATIONS/Strike online exhibition. Project focused on converting Toya's original desktop application into a web application.</summary></entry><entry><title type="html">Shinji Toya - Paint Your Face Away</title><link href="http://localhost:4000/artist-res/2020/08/17/shinji-paint-face.html" rel="alternate" type="text/html" title="Shinji Toya - Paint Your Face Away" /><published>2020-08-17T13:00:00+00:00</published><updated>2020-08-17T13:00:00+00:00</updated><id>http://localhost:4000/artist-res/2020/08/17/shinji-paint-face</id><content type="html" xml:base="http://localhost:4000/artist-res/2020/08/17/shinji-paint-face.html">&lt;h1 id=&quot;recent-work&quot;&gt;Recent Work&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/img/pyfa/shinji-web-site-fullwidth.png&quot; alt=&quot;alt-text&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Over the summer Alejandro lent a hand to help develop &lt;a href=&quot;https://shinjitoya.com/&quot;&gt;Shinji Toya&lt;/a&gt; latest interactive art project.
The project is called &lt;a href=&quot;https://paintyourfaceaway.net&quot;&gt;Paint Your Face Away&lt;/a&gt; and invites the audience to distort their own self portrait using face-painting and masking techniques. We have been friends wth Shinji for a few years now and the project has had some different iterations in the form of workshop sessions so it was neat that he asked us if we could help with some aspects of the web development to turn it into a web application.&lt;/p&gt;

&lt;p&gt;Through interacting with the tool audiences are able to learn when a face is no longer a face in the eyes of facial recognition software and contribute to an alternative facial database of adverserial self-portraits.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/pyfa/shinji1.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/pyfa/shinji2.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="artist-res" /><category term="ML" /><category term="web" /><summary type="html">Over the summer Alejandro lent a hand to help develop Shinji Toya latest interactive art project, Paint Your Face Away.</summary></entry><entry><title type="html">Christopher Macinnes - Artist Residency Program 2020</title><link href="http://localhost:4000/artist-res/2020/08/04/Chris-Macinnes.html" rel="alternate" type="text/html" title="Christopher Macinnes - Artist Residency Program 2020" /><published>2020-08-04T13:00:00+00:00</published><updated>2020-08-04T13:00:00+00:00</updated><id>http://localhost:4000/artist-res/2020/08/04/Chris-Macinnes</id><content type="html" xml:base="http://localhost:4000/artist-res/2020/08/04/Chris-Macinnes.html">&lt;p&gt;&lt;img src=&quot;/img/res01/rpg2.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are very pleased to announce &lt;a href=&quot;http://Christophertophermacinnes.com/&quot;&gt;Christopher Macinnes&lt;/a&gt; as the first artist to take part in our (online) residency program for 2020. Christopher was selected through our collaboration with &lt;a href=&quot;http://www.offsiteproject.org/&quot;&gt;Offsite Projects&lt;/a&gt; where he presented &lt;a href=&quot;http://www.offsiteproject.org/ZIP&quot;&gt;Imagine escape in darkness&lt;/a&gt; which you can dowload and view offline as part of their ZIP residency program.&lt;/p&gt;

&lt;p&gt;Christopher MacInnes is an artist based in London. Taking computing and networks as a starting point he works with software, hardware and occasionally organisms. Using the diverse vectors of our planetary networks, MacInnes attempts to trace the mycellenic tangle of chaotic phenomena across platforms, landscapes and bio-synthetic ecologies.&lt;/p&gt;

&lt;p&gt;We have begun working with Christopher to help develop and realise a multiplayer online game in which the audience will explore a an imagined world full of mystical artefacts and aspects of techno-spirtuality and gnosticism. The game is already looking really exciting, see some of the images below and Christopher has been developing the environment using three.js, websockets and node and we will hopefully be sharing some technical guides to accompany the new work for those interested very soon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res01/rpg1.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="artist-res" /><category term="web" /><summary type="html">We are very pleased to announce Christopher Macinnes as the first artist to take part in our (online) residency program for 2020!</summary></entry><entry><title type="html">Residency Reflections: Tamara Kametani</title><link href="http://localhost:4000/artist-res/2019/12/10/tamara-kametani-google-car.html" rel="alternate" type="text/html" title="Residency Reflections: Tamara Kametani" /><published>2019-12-10T13:00:00+00:00</published><updated>2019-12-10T13:00:00+00:00</updated><id>http://localhost:4000/artist-res/2019/12/10/tamara-kametani-google-car</id><content type="html" xml:base="http://localhost:4000/artist-res/2019/12/10/tamara-kametani-google-car.html">&lt;h3 id=&quot;chasing-the-google-maps-car-by-alejandro-ball&quot;&gt;Chasing the Google Map’s Car by Alejandro Ball&lt;/h3&gt;

&lt;h2 id=&quot;artist-residency&quot;&gt;Artist Residency&lt;/h2&gt;

&lt;h3 id=&quot;the-beginning&quot;&gt;The beginning&lt;/h3&gt;
&lt;p&gt;Tamara had a very clear concept and roadmap for the project she wanted to undertake during her residency time with Agorama. The concept… locate Google Street View recording car and secretly insert herself into Google Map.&lt;/p&gt;

&lt;p&gt;The idea originated from an earlier moment when Tamare was still attending her MA with the Royal Academy of Art (London, UK), where she described a moment when she was walking to class and suddenly realised the famous Google Street View car had passed her. In her curiousity, Tamara explored Google Maps in the location of her interaction to discover that she had in fact been captured serindipitously and now was a permenant main stay of the world’s largest digital interactive map.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect2/tamara-beginning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was with this experience in mind that Tamara approach the Agorama residency; an attempt to reenact this previous experience through the residency process.&lt;/p&gt;

&lt;h3 id=&quot;residencys-approach&quot;&gt;Residency’s approach&lt;/h3&gt;

&lt;p&gt;As for the approach to Tamara’s residency, this brough Agorama into a position of researcher and proposing various tools and platforms that could possible help Tamara achieve her vision.&lt;/p&gt;

&lt;p&gt;In our first initial meeting we discussed several options for tracking her progress in searching out the Google Street View car’s route, from using GPS tracking devices to platforms where she could document the progress of her project. In the end Tamara settled on using Google’s own &lt;a href=&quot;https://mymaps.google.com&quot;&gt;My Maps&lt;/a&gt;, which allows for a user to create their own Google map with landmarks, icon placement and the highlighing of walking and driving routes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect2/Kametani_T_01.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also pursued other options in the case of Tamara failing to find he Google Street View car through the means of other potential surveillance material. One particular example which was attempted and not used for the final work of art is the &lt;a href=&quot;https://api.tfl.gov.uk/Place/Type/JamCam&quot;&gt;TFL Jam Cam public API database&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The final elements, which were enacted for Tamara in her project’s endeavour were the create’s of a closed Whatsapp group of trusted individuals and a meeting with one of the Google Street View car’s managers. In the former case, participants that were invited to the closed Whatsapp group were instructed to keep an eye out for the Google Street View car and to photograph it then share it with the group along with the location of the sighting.&lt;/p&gt;

&lt;p&gt;In the latter action, Agorama were able - through insider connections ;) - to score a meeting with a Google Street View car manager. Due to this sensitive nature this is all that will be said on the matter.&lt;/p&gt;

&lt;h2 id=&quot;processes-overview&quot;&gt;Processes overview&lt;/h2&gt;

&lt;h3 id=&quot;tfl-jam-cam-api&quot;&gt;TFL Jam Cam API&lt;/h3&gt;
&lt;p&gt;While not included into the final project, this aspect of the collaboration was incredibly fun to put together for Tamara! Essentially it was agreed with Tamara that I would create a simple program that would repeatedly download cctv footage from TFL’s Jam Cam API within a 5 mile radius of Tamara’s Google Street View car search route.&lt;/p&gt;

&lt;p&gt;To provide context into the TFL API… the City of London is known as one of the worlds most surveilled cities in the world, with thousands of cameras scattered across the city. The interesting aspect in all this &lt;code class=&quot;highlighter-rouge&quot;&gt;Big Borther&lt;/code&gt; business is that these thousands of CCTV cameras are not under the control of a single party, and in the case of the CCTV camera attempted to be used for Tamara’s project TFL controls roughly 911 different CCTV &lt;code class=&quot;highlighter-rouge&quot;&gt;Jam Cams&lt;/code&gt; for traffic surveillance purposes. Thus, it was completely legally to work with this online data set (at least at the time)!&lt;/p&gt;

&lt;p&gt;Due to the way that footage is captured for the API, it was necessary to create a scrapping system that would repeatedly query the API data set. The reason for this was because TFL only uploads 5 second video clips of its CCTV footage every 5 mins. Thus, the approach, as seen below, was to sort through the data and discover which cameras were in the 5 mile radius that Tamara required. From there is the dataset matches our location criteria, we package the data we want into a json object. From this point the next step is to create a &lt;code class=&quot;highlighter-rouge&quot;&gt;fetch()&lt;/code&gt; request to the server we want to send the data files to.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PLEASE NOTE: to attempt this your are required to have a working server that you control with this required storage space and API receiver system inside that server&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;At this point all we need to do is create this simple scrapping function into an interval function (meanign a function that runs periodically using the &lt;code class=&quot;highlighter-rouge&quot;&gt;setinterval()&lt;/code&gt; js function), and tada! You have a CCTV scrapping system that will bestow you with a huge data dump of Jam Cam footage.&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jamcall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;ev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;preventDefault&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

          &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;https://api.tfl.gov.uk/Place/Type/JamCam&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;51.4228&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;51.5967&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3891&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1531&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;modified&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

                &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
                  &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;media&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

                &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;http://target.API.receiver/&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;cors&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;omit&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Content-Type&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;application/json&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

              &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

          &lt;span class=&quot;nx&quot;&gt;setInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;https://api.tfl.gov.uk/Place/Type/JamCam&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;51.4228&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;51.5967&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3891&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1531&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;modified&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

                  &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
                    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;media&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

                  &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;http://target.API.receiver/&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;cors&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;omit&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Content-Type&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;application/json&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

                &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If you would like to see that actual data dump of this process, please follow this link: &lt;a href=&quot;http://cctv-dump.agorama.org.uk/&quot;&gt;Agorama TfL datadump&lt;/a&gt;.
Currently, we only have on display the actual day that Tamara request this program to be used.&lt;/p&gt;

&lt;h3 id=&quot;closed-whatsapp-google-street-view-car-search-group-gallery&quot;&gt;Closed Whatsapp Google Street View car search group (Gallery)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-3.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-5.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;tamara-kametanis-my-google-map&quot;&gt;Tamara Kametani’s My Google Map&lt;/h3&gt;

&lt;iframe src=&quot;https://www.google.com/maps/d/embed?mid=1rFaSi2vcBy80ss-csdHdD8Kx9hUYFsGO&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/iframe&gt;</content><author><name></name></author><category term="artist-res" /><category term="web" /><category term="code" /><summary type="html">Agorama's residency reflection focusing on Tamara Kametani's commission. Swayze Effect. By Alejandro Ball.</summary></entry><entry><title type="html">Residency Reflections: Hazel Brill</title><link href="http://localhost:4000/artist-res/2019/12/09/hazel-bill-ml-agents.html" rel="alternate" type="text/html" title="Residency Reflections: Hazel Brill" /><published>2019-12-09T13:00:00+00:00</published><updated>2019-12-09T13:00:00+00:00</updated><id>http://localhost:4000/artist-res/2019/12/09/hazel-bill-ml-agents</id><content type="html" xml:base="http://localhost:4000/artist-res/2019/12/09/hazel-bill-ml-agents.html">&lt;h1 id=&quot;machine-learning--unity-by-max-dovey&quot;&gt;Machine Learning &amp;amp; Unity by Max Dovey&lt;/h1&gt;

&lt;h2 id=&quot;artist-residency&quot;&gt;Artist Residency&lt;/h2&gt;

&lt;p&gt;Hazel Brill is an artist based in London who graduated from the Slade School of Art in 2017, where I first incidentally became captivated in her complacently titled ‘I made a show for you’ (2017). Hazel creates large scale diorama’s to host her videos, characters and narrations that are all projected onto 3d objects. She uses projection mapping to effectively create highly captivating 3d physical environments that the audience can inhabit without the need for any virtual reality goggles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/outtosea0028-small.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As part of Hazel’s residency she had expressed an interest in how artificial intelligence can be used to automate or generate autonomous agents and characters. In one of our initial conversations we spent some time discussing how robotics, animatronics and neural networks relate to one another and indulging in speculating about how something like MIT’s AlterEgo project actually works (Spoiler – we have no idea). We looked at many ‘performances’ of artificial intelligent systems from animatronics to robotics and neural network interfaces - While fascinated by the detail on some of the latest animatronic models we also had to work within our resources of time and budget. Hazel often created characters or bodies that communicate through sounds in her work and we began to discuss how those bodies could be implanted with some sort of artificial intelligence to behave or act autonomously.&lt;/p&gt;

&lt;h2 id=&quot;technical-process&quot;&gt;Technical Process&lt;/h2&gt;

&lt;h3 id=&quot;aim--to-create-autonomous-animations-by-connecting-machine-learning-models-to-unity&quot;&gt;Aim – to create autonomous animations by connecting machine learning models to Unity&lt;/h3&gt;

&lt;h4 id=&quot;software-requirements&quot;&gt;software requirements&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;python 3.6&lt;/li&gt;
  &lt;li&gt;TensorFlow&lt;/li&gt;
  &lt;li&gt;Unity&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;useful-resources&quot;&gt;useful resources&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Unity-Technologies/ml-agents&quot;&gt;Unity Machine Learning Toolkit (beta)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-by-step-guide&quot;&gt;Step-by-step guide&lt;/h3&gt;

&lt;p&gt;Setting up your working environments&lt;/p&gt;

&lt;p&gt;1) Install unity according to your specific operating system (linux users are advised to use unityHub as it maintains the latest version)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/@indiecontessa/setting-up-a-python-environment-with-TensorFlow-on-macos-for-training-unity-ml-agents-faf19d71201&quot;&gt;guide for Mac OSX&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/@tijmenlv/setting-up-ml-agents-on-linux-82972c353ad7&quot;&gt;guide for Ubuntu 16.04&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) Install the machine learning dependencies&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We recommend using anaconda with python 3.6&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cse.unsw.edu.au/~en1811/resources/getting-started/install-anaconda.html&quot;&gt;anaconda installation guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alternatively, if you already have your own environment with python 3.6 installed you can skip installing an enconda environment and go straight to installing the dependencies for &lt;a href=&quot;&quot;&gt;ml-agents toolkit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;using pip3&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  install_requires=[
        &quot;mlagents_envs==0.8.2&quot;,
        &quot;tensorflow&amp;gt;=1.7,&amp;lt;1.8&quot;,
        &quot;Pillow&amp;gt;=4.2.1&quot;,
        &quot;matplotlib&quot;,
        &quot;numpy&amp;gt;=1.13.3,&amp;lt;=1.14.5&quot;,
        &quot;jupyter&quot;,
        &quot;pytest&amp;gt;=3.2.2,&amp;lt;4.0.0&quot;,
        &quot;docopt&quot;,
        &quot;pyyaml&quot;,
        &quot;protobuf&amp;gt;=3.6,&amp;lt;3.7&quot;,
        &quot;grpcio&amp;gt;=1.11.0,&amp;lt;1.12.0&quot;,
        'pypiwin32==223;platform_system==&quot;Windows&quot;',
    ],
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3) Virtual Environment&lt;/p&gt;

&lt;p&gt;use conda to create a virtual environment to run your ml-agents toolkit from (We created it with)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda create --name tensor python=3.6 pip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is located in Documents/unity/ml-agents-master/&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda activate tensor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to close we then type&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) Install the ML-Agent toolkit&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git clone https://github.com/Unity-Technologies/ml-agents.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;training-models-with-unity--ml-agents&quot;&gt;Training Models with Unity &amp;amp; ML-Agents&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/ml-agents-workflow1.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These terms won’t mean much until you start working but its good to introduce the ‘abstract’ concepts of how you are going to interface between TensorFlow &amp;gt; python &amp;gt; Unity.&lt;/p&gt;

&lt;p&gt;Three important things to remember:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;You need an academy to run the simulation&lt;/li&gt;
  &lt;li&gt;Within the the academy you need a brain object&lt;/li&gt;
  &lt;li&gt;Inside the brain you need to inject a (pre-trained) model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/workflow2.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml-1.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;1-open-up-unity&quot;&gt;1) Open up Unity&lt;/h5&gt;

&lt;p&gt;Now you have successfully setup your environments you are ready to begin using the ml-agents toolkit with unity.&lt;/p&gt;

&lt;p&gt;Before training your own model its handy to examine the pre-fabs and demos that are available within the unity plugins manager within your unity program drag the complete Ml-agents folder into your assets folder within your project console this should import all the assets you need.&lt;/p&gt;

&lt;p&gt;Navigate to ML-agents in your Project Assets Window &amp;gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml-2.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;2-import-the-ml-agents-tutorials&quot;&gt;2) Import the ML-Agents Tutorials&lt;/h5&gt;

&lt;p&gt;For this tutorial, we are going to use the crawler example so navigate to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ML-agents &amp;gt; examples &amp;gt; Crawler &amp;gt; Scenes&amp;gt;
and launch the ‘Crawler Dynamic Scene’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;this is going to generate a scene with multiple crawlers, you can go ahead and delete the 10 other crawlers for this tutorial as it will load faster&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml-2.0.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;3-give-your-spider-a-brain&quot;&gt;3) Give your spider a brain&lt;/h5&gt;

&lt;p&gt;In ML-agents &amp;gt; examples &amp;gt; Crawler &amp;gt; Brains&lt;/p&gt;

&lt;p&gt;select CrawlerDynamicLearning.asset and drag into your object panel (on the left)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml3.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;4-give-your-brain-a-pre-trained-model&quot;&gt;4) Give your brain a pre-trained model&lt;/h5&gt;

&lt;p&gt;This should now be visible and listed as ‘Academy’ layer, with the academy layer selected you need to give it a brain!&lt;/p&gt;

&lt;p&gt;Double click on the brain , this will bring up a new inspector window&lt;/p&gt;

&lt;p&gt;in ML-agents &amp;gt; examples &amp;gt; Crawler &amp;gt; TFModels&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml-4.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;drag CrawlerDynamicLearning onto the brain&lt;/p&gt;

&lt;p&gt;this is a pretrained neural network model that has been trained to repeatedly follow the target (the yellow / orange box)&lt;/p&gt;

&lt;p&gt;once you have an academy layer, with a brain, with a pre-trained neural network in it you can hit the play button and watch the crawler in action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/spider-crawl.gif&quot; style=&quot;display:block;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;going-further&quot;&gt;Going Further&lt;/h2&gt;

&lt;p&gt;Now you have figured out how to use the pre-trained models, you can then begin customising the TensorFlow models and reward functions to create your own models.&lt;/p&gt;

&lt;p&gt;Working within your virtual environment in your terminal window&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-mlagents.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Go to utlities open up ‘terminal’&lt;/li&gt;
  &lt;li&gt;Copy into terminal or type
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd  your/working/Directory/unity/ml-agents-master/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;From here you should activate your working environment
copy into terminal or type
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda activate tensor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Test that your ml-agents is installed by typing
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ mlagents-learn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To train your own own model use this line below and hit the ‘play’ button when it asks….&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ mlagents-learn /Users/hazelbrill/Documents/unity/ml-agents-master/config/trainer_config.yaml --run-id=firstRun --train
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From here you should be able to see the training process in the unity editor window. To terminate the training press Ctrl C in the terminal window to stop the process.
the model should be saved and available to you within the Unity editor.
You can edit and change the training settings with the trainer_config.yaml&lt;/p&gt;

&lt;h5 id=&quot;after-training&quot;&gt;After training&lt;/h5&gt;
&lt;p&gt;You can press Ctrl+C to stop the training, and your trained model will be at &lt;code class=&quot;highlighter-rouge&quot;&gt;models/&amp;lt;run-identifier&amp;gt;/&amp;lt;brain_name&amp;gt;.nn&lt;/code&gt; where &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;brain_name&amp;gt;&lt;/code&gt; is the name of the Brain corresponding to the model. (Note: There is a known bug on Windows that causes the saving of the model to fail when you early terminate the training, it’s recommended to wait until Step has reached the max_steps parameter you set in trainer_config.yaml.) This file corresponds to your model’s latest checkpoint. You can now embed this trained model into your Learning Brain by following the steps below, which is similar to the steps described above.&lt;/p&gt;

&lt;h5 id=&quot;helpful-resources&quot;&gt;helpful resources&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md&quot;&gt;https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gym.openai.com/&quot;&gt;https://gym.openai.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FXFrwvV60N4&amp;amp;list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&amp;amp;index=3&quot;&gt;https://www.youtube.com/watch?v=FXFrwvV60N4&amp;amp;list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&amp;amp;index=3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Unity ML tutorial - &lt;a href=&quot;https://www.youtube.com/watch?v=x2RBxmooh8w&quot;&gt;https://www.youtube.com/watch?v=x2RBxmooh8w&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Another series - &lt;a href=&quot;https://www.youtube.com/watch?v=uiutRBXfEbg&amp;amp;list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&amp;amp;index=2&quot;&gt;https://www.youtube.com/watch?v=uiutRBXfEbg&amp;amp;list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&amp;amp;index=2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=x2RBxmooh8w&quot;&gt;https://www.youtube.com/watch?v=x2RBxmooh8w&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/?_ga=2.221967515.2097880843.1563531955-2130937146.1561465028&quot;&gt;https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/?_ga=2.221967515.2097880843.1563531955-2130937146.1561465028&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/&quot;&gt;https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="artist-res" /><category term="ML" /><category term="code" /><summary type="html">Agorama's residency reflection focusing on Hazel Brill's commission. Swayze Effect. By Max Dovey.</summary></entry><entry><title type="html">Swayze effect</title><link href="http://localhost:4000/projects/2019/11/01/swayze-effect.html" rel="alternate" type="text/html" title="Swayze effect" /><published>2019-11-01T11:00:00+00:00</published><updated>2019-11-01T11:00:00+00:00</updated><id>http://localhost:4000/projects/2019/11/01/swayze-effect</id><content type="html" xml:base="http://localhost:4000/projects/2019/11/01/swayze-effect.html">&lt;p&gt;&lt;img src=&quot;/img/projects/swayze/swayze-4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hazel Brill, James Irwin and Tamara Kametani were invited to use Agorama’s studio at Raven Row from April 2019 to conceive new work informed by technology. Through collaboration and computer programming, this residency aimed to provide the artists with access to digital tools for the development of their practices, culminating in the physical exhibition Swayze Effect.&lt;/p&gt;

&lt;p&gt;Using emerging technologies such as augmented reality, machine learning and digital mapping, the artists trace, transform and track bodily movements from the physical to the digital. The term ‘Swayze effect’ is used in virtual reality design to describe the struggle of affecting virtual or digital environments with embodied feedback and physical presence. All the works in the exhibition suggest an intervention into an interface – possible ways of navigating and experiencing the world around us through augmented viewfinders. &lt;/p&gt;

&lt;p&gt;Hazel Brill used her residency to experiment with animating characters, applying machine learning and game engines to generate locomotion processes such as climbing and crawling. She invites the viewer to step into a rendered plateau inhabited only by biomorphic organisms that move, interact and grow through reinforcement learning algorithms. Tamara Kametani has been attempting to hunt down a Google Street View car, inspired by a previous accidental encounter with it in 2017. In an effort to assert some form of control over her virtual depiction, the artist used her body as a disruptive tool, devising an on-going game of deception with Google in order to ‘hack’ its global scope in the most analogue way possible – physical human interference. James Irwin makes use of augmented reality to delineate the space between bodies in physical spaces and their digital facsimiles, questioning the fragmentation of identity in the virtual realm. Throughout his residency, Irwin used video to document his reflection and manipulated those recordings through various filter degradation effects, simulating an augmented reality environment to encapsulate digital loss. By inviting the viewer to experience his robotic mirror sculpture, Irwin aims to trap one’s gaze in this reflective mise en abyme – a corroded landscape.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/swayze/swayze-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/img/projects/swayze/swayze-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/img/projects/swayze/swayze-3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Exhibition hosted by Platform Southwark, September 2019.&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Swayze effect presents the works produced as a result of Agorama’s 2019 Artistic Residencies Programme. The term ‘Swayze effect’ is used in virtual reality design to describe the struggle of affecting virtual or digital environments with embodied feedback and physical presence. Exhibiting artists - Hazel Brill, James Irwin and Tamara Kametani. Exhibition hosted by Platform Southwark.</summary></entry><entry><title type="html">The Server Co-op</title><link href="http://localhost:4000/projects/2019/06/30/coop-project-overview.html" rel="alternate" type="text/html" title="The Server Co-op" /><published>2019-06-30T11:00:00+00:00</published><updated>2019-06-30T11:00:00+00:00</updated><id>http://localhost:4000/projects/2019/06/30/coop-project-overview</id><content type="html" xml:base="http://localhost:4000/projects/2019/06/30/coop-project-overview.html">&lt;p&gt;&lt;img src=&quot;/img/projects/co-op/coop-1.jpg&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We organised public workshops at Mozilla festival, free university and some european universitys and colleges. The research manifested in a number of creative applications and interpretation of p2p internet culture and some documented code for turning raspberry pi’s into p2p web servers that can be found on our github page and the rest of the research resides here.&lt;/p&gt;

&lt;p&gt;We host regular p2p web meet-ups at our studio at Raven Row for anyone interested in the distributed internet. The aim is to facilitate a community interested in experimenting, developing and maintaining a distributed internet infrastructure. We have been invited to give workshops and lead regular educational programs exploring the potential for a self organized distributed hosting platform for Agorama.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/co-op/coop2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Agorama established and hosted a working research group into the distributed p2p (peer-to-peer) web at their studio at Raven Row between Sep. 2018 - Jun. 2019. The aim was to foster a community interested in experimenting, developing and maintaining a distributed Internet infrastructure.</summary></entry><entry><title type="html">Moses the Lonely Londoner</title><link href="http://localhost:4000/projects/2019/05/30/moses.html" rel="alternate" type="text/html" title="Moses the Lonely Londoner" /><published>2019-05-30T11:00:00+00:00</published><updated>2019-05-30T11:00:00+00:00</updated><id>http://localhost:4000/projects/2019/05/30/moses</id><content type="html" xml:base="http://localhost:4000/projects/2019/05/30/moses.html">&lt;p&gt;&lt;img src=&quot;/img/projects/moses/moses-4.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Moses the Lonely Londoner is a project set on investigating the concept of intimate digital spaces, namely that of the telecocoon which was first detailed in Japanese teen keitai culture as an invisible space of intimacy linked together by an individual’s mobile device.&lt;/p&gt;

&lt;p&gt;The project takes inspiration from author Sam Selvon’s novel The Lonely Londoners and makes manifest its main character, Moses Aloetta, through the use of Artificial Intelligence. Drawing parallels between the loneliness expressed in Selvon’s 1950s London and the notions of isolation propelled by Social Media, users are invited to connect with Moses on their mobile device and enter his telecocoon.&lt;/p&gt;

&lt;p&gt;This project was exhibited in the Photographers’ Gallery as part of the exhibition “For the Time Being”, and was commissioned by the Royal College of Art in partnership with the Photographers’ Gallery.&lt;/p&gt;

&lt;p&gt;For the Time Being was a group exhibition curated by six postgraduate students seeking to explore the shifting responsibilities of institutions in this networked age, as part of the MA Curating Contemporary Art Programme Graduate Projects 2019 at Royal College of Art, London. The exhibition was produced in collaboration with The Photographers’ Gallery in May 2019.&lt;/p&gt;

&lt;p&gt;Links…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/moses/moses-3.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/moses/moses-1.jpg&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/moses/moses-2.jpg&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Moses the Lonely Londoner is a project set on investigating the concept of intimate digital spaces linked together by an individual's mobile device. This project was exhibited in the Photographers' Gallery as part of the exhibition &quot;For the Time Being&quot;, and was commissioned by the Royal College of Art in partnership with the Photographers' Gallery.</summary></entry><entry><title type="html">p2p Web Jam 2</title><link href="http://localhost:4000/education/2019/04/27/p2pwebjam2.html" rel="alternate" type="text/html" title="p2p Web Jam 2" /><published>2019-04-27T12:50:01+00:00</published><updated>2019-04-27T12:50:01+00:00</updated><id>http://localhost:4000/education/2019/04/27/p2pwebjam2</id><content type="html" xml:base="http://localhost:4000/education/2019/04/27/p2pwebjam2.html">&lt;p&gt;&lt;img src=&quot;/img/webjam2/wide.jpg&quot; alt=&quot;alt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In order to make contributing to the infastructure of the p2p web as easy as possible we are collating resources to create simplified guides to installing Homebase (a dat server) on a Raspberry pi.&lt;/p&gt;

&lt;p&gt;On Saturday 27th April 2019 we gathered at the studio to walk through the installation process of turning a Raspberry pi Model 3 into a homebase server. We have already done this on a few pis in our studio (see this &lt;a href=&quot;_posts/2018-11-07-dat-server-node-tutorial.md&quot;&gt;post&lt;/a&gt; but we were looking to streamline the process so we could deploy this custom p2p web server stack to multiple pis. As always our attitude was to learn through doing and through an open workshop that anyone could drop in and learn about configuring their raspberry pis into p2p web servers.&lt;/p&gt;

&lt;p&gt;One of our server co-op members Piper has published this very good &lt;a href=&quot;https://piperhaywood.com/raspberry-pi-homebase-dat/&quot;&gt;read me&lt;/a&gt; which we have &lt;del&gt;copied&lt;/del&gt;  pasted from to ensure this could help others repeat the process. The  guide assumes you already have a raspberry pi with raspbian installed but IF not, do not worry, visit Piper’s notes &lt;a href=&quot;https://piperhaywood.com/raspberry-pi-homebase-dat/&quot;&gt;here&lt;/a&gt; or start here from 0.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/webjam2/aims2.jpg&quot; alt=&quot;alt&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;aims---to-configure-a-streamlined-install-process-for-batch-deployment-of-homebase-servers-on-the-raspberry-pi-3-model-b&quot;&gt;AIM(S) - to configure a streamlined install process for batch deployment of homebase servers on the raspberry pi 3 model B.&lt;/h2&gt;

&lt;p&gt;Requirements&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Raspberry pi 3 Model B&lt;/li&gt;
  &lt;li&gt;SD card with raspbian installed&lt;/li&gt;
  &lt;li&gt;HDMI Monitor&lt;/li&gt;
  &lt;li&gt;Keyboard + Mouse&lt;/li&gt;
  &lt;li&gt;Wifi + electricity&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/webjam2/3.jpeg&quot; alt=&quot;alt&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;How many heads are required before you can login headless to your raspberry pi?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-0-install-raspbian-on-sd-card-&quot;&gt;Step 0 Install Raspbian on SD Card-&lt;/h2&gt;
&lt;p&gt;If you have yet to turn on your raspberry pi for the first time you must first do some preliminary steps.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A) Flash Raspbian onto an SD card using a program called &lt;a href=&quot;https://www.balena.io/etcher/&quot;&gt;etcher&lt;/a&gt; or using your command line ‘DD’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dd bs=4M if=2018-11-13-raspbian-stretch.img of=/dev/SDX conv=fsync status=progress&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;please ensure you have selected the correct directory as flashing will remove all data from disk
&lt;a href=&quot;https://www.raspberrypi.org/documentation/installation/installing-images/linux.md&quot;&gt;Reference&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;B) Connect Raspberry Pi to WiFi on the command line&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;With your SD card that now has Raspbian installed, plug into a computer (using a micro SD card reader) and you want to edit ‘wpa_supplicant.conf’ file to add the details for the wifi network you want the raspberry pi to connect to&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;with a text editor open up &lt;code class=&quot;highlighter-rouge&quot;&gt;/Volumes/boot/wpa_supplicant.conf&lt;/code&gt; if this does not yet exist create a file using the Command Line &lt;code class=&quot;highlighter-rouge&quot;&gt;nano /Volumes/boot/wpa_supplicant.conf&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add your wifi information in here and copy this into your wpa_supplicant.conf file&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;country=gb
update_config=1
ctrl_interface=/var/run/wpa_supplicant

network={
  scan_ssid=1
  ssid=&quot;YOUR_NETWORK_NAME&quot;
  psk=&quot;YOUR_NETWORK_PASSWORD&quot;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;once this is done, make sure to save the file and eject the mounted disk properly to avoid any damage.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;C) Enable SSH on the Raspberry pi&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;plug your SD card with raspbian mounted onto your computer&lt;/li&gt;
  &lt;li&gt;open the command line and enter
&lt;code class=&quot;highlighter-rouge&quot;&gt;touch /Volumes/boot/ssh
&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;This will create a new empty file titled ssh in the root of your SD card. This empty file will allow you to connect via SSH when the Pi is first booted.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;D) Log in to a Raspberry Pi via SSH as the root user pi
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh pi@raspberrypi.local
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;if your having trouble connecting to your Raspberry pi, try pinging it to see if it has successfully connected to the network
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ping raspberrypi.local
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;You can also view all the devices on a network with a handy mobile application called &lt;a href=&quot;https://www.fing.com/&quot;&gt;fing&lt;/a&gt;. This app will show all the devices on the pi and their ip address. If your pi has successfully connected to the wifi network it will be listed in this app. you can then ssh into with the command&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ssh pi@192.172.3.4
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
    &lt;p&gt;&lt;img src=&quot;/img/webjam2/ssh-pi2.png&quot; /&gt;
&lt;!-- ![](/img/webjam2/ssh-pi2.jpg) --&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;step-1-use-ansible-to-configure-your-raspberry-pi-server&quot;&gt;Step 1 Use Ansible to Configure your Raspberry Pi Server&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/agoramaHub/ansible-raspberry-server&quot;&gt;Agorama’s Ansible Raspberry server playbook&lt;/a&gt; automates a number of fiddly tasks that are required to get a Raspberry Pi set up as a server geared towards use with Homebase. You can get a feel for the tasks that will be performed by the playbook by browsing the files within the playbook, working backwards from all.yml.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is a fork of &lt;a href=&quot;https://github.com/lachenmayer&quot;&gt;Harry Lachenmayer’s Raspbery pi Ansible server&lt;/a&gt; that automates alot of the heavy lifting when configuring rpis. e.g. installing nginx, node, DNS and lots more. Each component is optional and can be run by selecting various playbooks that are stored as .YML files.&lt;/p&gt;

&lt;p&gt;Once you follow the steps on this github you should have a raspberry pi with node and nginx installed. Now its time for fun part ;-)&lt;/p&gt;

&lt;h2 id=&quot;step-2-install-dat-and-run-homebase-on-a-raspberry-pi-server&quot;&gt;Step 2 Install DAT and run Homebase on a Raspberry Pi server&lt;/h2&gt;
&lt;p&gt;For security purposes, the Ansible playbook configures worker user on Raspberry server so that we’re not using the root user pi to install and run software. When you first log in with SSH you are logged in as the root user, so we need to switch to worker by running:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A) Change user to the worker user that was set-up to run the ansible playbooks
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo su worker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;B) Install Dat
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm install -g dat
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;C) Test whether or not the dat installation works with the Pi configuration by running:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dat doctor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;D) Install homebase by running:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;npm install -g @beaker/homebase
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;E) Change directory to the user root:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cd
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;F) open up the Homebase config with nano, then paste in:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-dats:&quot;&gt;  - url: dat://01cd482f39eb729cdcbb479b03b0c76c6def9cfc9cff276a564a17c99c4432f4/
  - url: dat://b0bc462c23e3ca1fee7731d0a1a2dc38bd9b9385daa413520e25aea0a26237a6/
  - url: dat://f707397e8dacc1893dced5afa285bab1715b70fe40135c2e14aac7de52f2c6bb/

directory: ~/.homebase        # where your data will be stored

# For API service. Establish API endpoint through port 80 (http)
ports:
  http: 8080                  # HTTP port for redirects or non-TLS serving
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Swapping the DAT Urls to DATS of your choice. this config YML file is extremely fussy about indentation so make sure everything is tidy before saving it!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;G) Now you can run Homebase from your CLI
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;homebase
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;H) H is for Homebase which should now display it is running with the following&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; ▶ Directory:    /home/pi/.homebase
 ▶ Ports:        8080 (HTTP) 443 (HTTPS)
 ▶ HTTP mirror:  ✔ enabled
 ▶ Lets Encrypt: ✖ disabled
 ▶ Dashboard:    ✖ disabled
 ▶ WebAPI:       ✔ enabled undefined

Serving
  dat://7aeb3df618daeb74c92ec11f3102e71c313e681e2febf957bef0095766318874/
  at
Serving
  dat://f707397e8dacc1893dced5afa285bab1715b70fe40135c2e14aac7de52f2c6bb/
  at
Serving
  dat://abb7e977f41cd11e0b58178256b9919004e7d6a96126c5822f42cddc67e43487
  at
Serving
  dat://1f1c214b103def29a0df591d1e335710cfa29a308d14ebc0b1e2e11778828663/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;img src=&quot;/img/webjam2/alegemma.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;step-3---turn-homebase-into-a-daemon-service&quot;&gt;STEP 3 - Turn Homebase into a daemon service&lt;/h2&gt;

&lt;p&gt;Daemonizing homebase means that it will constantly run in the background as long as the service hasn’t failed, the server is on, and the server is connected to the internet.This is important because the whole point is that we want the Dat sites that are listen in .homebase.yml to run in perpetuity. There are multiple ways to do this (Cron, rc.local, pm2) but we have found the most reliable is with &lt;a href=&quot;https://en.wikipedia.org/wiki/Systemd&quot;&gt;Systemd&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;A) add a service configuration for homebase. As the root user pi, run:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo nano /etc/systemd/system/homebase.service
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;B) in this file copy and paste the following:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[Unit]
Description=homebase

[Service]
Type=simple
ExecStart=/usr/bin/env .npm-packages/bin/homebase
WorkingDirectory=/home/worker/
Restart=on-failure
StandardInput=null
StandardOutput=syslog
StandardError=syslog
Restart=always
SyslogIdentifier=homebase
User=worker
Group=worker

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;save and exit the file.&lt;/p&gt;

&lt;p&gt;E) to start and stop the service type:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo service homebase start
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo service homebase stop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;F) If you make changes to the homebase.service file, you must reload the system.d service for the changes to take effect&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;systemctl daemon-reload
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Congratulations, you should now have &lt;a href=&quot;https://github.com/beakerbrowser/homebase&quot;&gt;homebase&lt;/a&gt; running as a service on your raspberry pi. This will help keep your DATS seeded more permanently. In the next guide we will introduce how to configure a Dynamic DNS so you can serve your DATS over HTTP and the p2p web.&lt;/p&gt;

&lt;p&gt;Thanks to &lt;a href=&quot;https://github.com/lachenmayer&quot;&gt;Harry Lachenmayer&lt;/a&gt; and &lt;a href=&quot;https://piperhaywood.com&quot;&gt;Piper Haywood&lt;/a&gt; for helping and everyone who attended the workshop&lt;/p&gt;</content><author><name></name></author><category term="education" /><category term="p2p" /><category term="web" /><summary type="html">Turning Raspberry pi into Homebase p2p web server</summary></entry></feed>