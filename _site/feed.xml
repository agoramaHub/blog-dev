<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-04-06T15:41:30+00:00</updated><id>/feed.xml</id><title type="html">Agorama</title><subtitle>Agorama main home page.</subtitle><entry><title type="html">Jonas Pequeno Artist residency program</title><link href="/artist-res/2021/04/06/jonas-pequeno-artist-residency-program.html" rel="alternate" type="text/html" title="Jonas Pequeno Artist residency program" /><published>2021-04-06T15:17:06+00:00</published><updated>2021-04-06T15:17:06+00:00</updated><id>/artist-res/2021/04/06/jonas-pequeno-artist-residency-program</id><content type="html" xml:base="/artist-res/2021/04/06/jonas-pequeno-artist-residency-program.html">&lt;p&gt;&lt;img src=&quot;/img/untitledblurry.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Residency Reflections on Jonas Pequeno’s residency ‘Gardens in Computers’&lt;/p&gt;

&lt;p&gt;Jonas has taken part in the Agorama X Offsite Residency program using the time to develop his archival practice and explore the application of databases in machine learning to reflect on the production of knowledge and representation in A.I. Jonas exerts the limits of language as an interface for classification and cascades through the parameters of logic used within A.I. architecture, exploring the cracks in semantics between human and computational reason to fall into absurd realms of reality.&lt;/p&gt;

&lt;p&gt;As part of the residency Jonas has been navigating representations of ‘nature’ from the perspective of the database, initially beginning with sound, Jonas indexed thousands of instances of representations of ‘nature’ from the user generated archives such as freesound.org, archive.org and flickr. The process of collating an extensive amount of media objects allowed Jonas to critically reflect on both the semantic boundaries of these data sets and the materiality of the data. The limits of language to classify natural phenomenon became central to the outputs of the residency, the sonic  artefacts of the sea transposed into artificially generated ambient shores, thousands of flower specimens re-grounded into 3d generated turf and portraits of the sky with clouds automatically removed.&lt;/p&gt;

&lt;p&gt;The French impressionist painter Henri Rousseau is most well known for his paintings of jungle scenes &amp;amp; detailed flora, but the artist’s knowledge of tropical plants came mostly from the botanical gardens in Paris as the artist is known to have never left his homeland France. The residency has been forced to be completed through various stages of lockdown, Jonas has similarly summoned an imagined notion of the term ‘nature’, from the position of an machine learning model that narrates its discoveries by continually parsing through symbolic meanings in a never ending daisy chain of potential understandings.&lt;/p&gt;</content><author><name></name></author><category term="artist-res" /><category term="ML" /><category term="web" /><summary type="html">Residency Reflections on Jonas Pequeno's residency 'Gardens in Computers'</summary></entry><entry><title type="html">Catinca Malaimare - Artist Residency Program 2020</title><link href="/artist-res/2020/11/12/catinca-malaimare.html" rel="alternate" type="text/html" title="Catinca Malaimare - Artist Residency Program 2020" /><published>2020-11-12T13:00:00+00:00</published><updated>2020-11-12T13:00:00+00:00</updated><id>/artist-res/2020/11/12/catinca-malaimare</id><content type="html" xml:base="/artist-res/2020/11/12/catinca-malaimare.html">&lt;p&gt;&lt;img src=&quot;/img/catinca-1.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;“On nights with enough lighting, we are illuminated by light reflected off the Cloud. Where the source of illumination is the dominant spectral line of gold-tinted light scattered at the highest luminance in our faces, we watch with an eye insensitive to other visual pollutants. The nighttime is tinged with yellow biased daylight, and we’re concealed and observant, corner-hidden in the darkest side of the glow dome.”&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Following on from our collaboration with Offsite Projects artist &lt;a href=&quot;https://www.catincamalaimare.com/&quot;&gt;Catinca Malaimare&lt;/a&gt; is are second artist in residence for our online residency program.&lt;/p&gt;

&lt;p&gt;For her ZIP residency in June Catinca produced ‘Lo Sodium’ a series of performative rituals that dramatised our complex companionship with technology through a series of chorographed interactions and a short story.  Catinca situates herself in and around a large pair of stage lights and invites the viewer to explore these majestic mechanical objects in a theatrical sense, delibreately demanding a discomforting intensity from the viewer and asking us to share the awkward physical presence of these regal (but sadly now redundant) tecnological artefacts.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/catinca-2a.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As part of her residency with us at Agorama, Catinca has been experimenting with two TV’s that rebelled against convention, the Philips Discoverer space helmet TV manufactured in the 1980s (see image) under the disguise that they are “worth looking at even when it’s off”. The uncomfortable reality that our machines are dance partners, closer than we are ready to admit, prompted us to work together on the construction of a pair of stage lights where bodies mimic one another.&lt;/p&gt;

&lt;p&gt;Catinca is regularly documenting the process on her &lt;a href=&quot;https://www.instagram.com/catincamalaimare/&quot;&gt;Instagram&lt;/a&gt; and the outputs will be shared publically as part of a group show early next year.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.catincamalaimare.com/&quot;&gt;Catinca Malaimare&lt;/a&gt; is a London based artist and current postgraduate student at The Royal Academy of Arts. Performing alongside anthropomorphised technologies, Malaimare’s choreography manifests our intimate relationship with photographic tools and the screens onto which they project our visages. Recent exhibitions, performances and screenings include 22.200 LEDs, Another Mobile Gallery, Bucharest; LO Sodium, ZIP Agorama X Off Site Project; RA Premiums Interim Show, The Royal Academy of Arts, London (2020); Bad Sights. Bad Sights Ahead, 16 Nicholson Street Gallery, Glasgow; Why Not Pour Video Into Our Eyes?, TUA Geidai, Tokyo; SUNs, STAMPEDE at Royal Academy of Arts; LAND GRAB, Sadie Coles HQ, London (2019); We are all involved in this mess, Enclave Lab; Prefix is post-, Lewisham Art House, London and Pilot Screens, P E R I C L O, Wrexham (2017). Forthcoming exhibitions include group show Dreams Made Flesh, Catinca Tabacaru Gallery, Bucharest (2021). Malaimare is founder and co-curator of Habeas Corpus (We Have A Body), a platform that aims to provide a testing ground for live and media-based works in contemporary performance art.&lt;/p&gt;</content><author><name></name></author><category term="artist-res" /><summary type="html">Catinca Malaimare brings her performative practice to Agorama's artist residency program</summary></entry><entry><title type="html">Shinji Toya, Paintyourfaceaway.net</title><link href="/projects/2020/08/30/paint-your-face-away.html" rel="alternate" type="text/html" title="Shinji Toya, Paintyourfaceaway.net" /><published>2020-08-30T11:00:00+00:00</published><updated>2020-08-30T11:00:00+00:00</updated><id>/projects/2020/08/30/paint-your-face-away</id><content type="html" xml:base="/projects/2020/08/30/paint-your-face-away.html">&lt;p&gt;Shinji Toya’s project, paint your face away, found its  start in life as a desktop application, which Shinji Toya had developed through the use of MAX/MSP visual programming language. Toya utilised this platform for several performative workshops, sharing with audiences and workshop goers the crucial discourse of artificial intelligence face tracking technology. Shinji Toya’s focus for the platform was to subvert the already rooted system of harvesting user’s face data, which is undertaken by most social media and technology platforms, by allowing users to upload an image of themselves and painting their face away, there by obscuring this data from AI harvesters.&lt;/p&gt;

&lt;p&gt;Agorama, was commissioned by Shinji Toya to aid in his platforms transformation from a desktop application to a fully realised web platform. Through this process Agorama, provided consultation services and full stack development work for Toya’s project including the identification of selecting and setting up the correct server setup, integrating face-tracking technology, and web-cam integration.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/paintface/paintface-1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/paintface/paintface-2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Paint Your Face Away was commission by Fotomuseum Winterthur and exhibited as part the museum’s &lt;a href=&quot;https://www.fotomuseum.ch/en/explore/situations/157307&quot;&gt;SITUATIONS/Strike&lt;/a&gt; online exhibition.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://paintyourfaceaway.net/&quot;&gt;paintyourfaceaway.net/&lt;/a&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Full stack development support and consultation for Shinji Toya latest interactive art project &quot;Paint Your Face Away&quot;. Project was commission by Fotomuseum Winterthur and exhibited as part the museum's SITUATIONS/Strike online exhibition. Project focused on converting Toya's original desktop application into a web application.</summary></entry><entry><title type="html">Shinji Toya - Paint Your Face Away</title><link href="/artist-res/2020/08/17/shinji-paint-face.html" rel="alternate" type="text/html" title="Shinji Toya - Paint Your Face Away" /><published>2020-08-17T13:00:00+00:00</published><updated>2020-08-17T13:00:00+00:00</updated><id>/artist-res/2020/08/17/shinji-paint-face</id><content type="html" xml:base="/artist-res/2020/08/17/shinji-paint-face.html">&lt;h1 id=&quot;recent-work&quot;&gt;Recent Work&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/img/pyfa/shinji-web-site-fullwidth.png&quot; alt=&quot;alt-text&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Over the summer Alejandro lent a hand to help develop &lt;a href=&quot;https://shinjitoya.com/&quot;&gt;Shinji Toya&lt;/a&gt; latest interactive art project.
The project is called &lt;a href=&quot;https://paintyourfaceaway.net&quot;&gt;Paint Your Face Away&lt;/a&gt; and invites the audience to distort their own self portrait using face-painting and masking techniques. We have been friends wth Shinji for a few years now and the project has had some different iterations in the form of workshop sessions so it was neat that he asked us if we could help with some aspects of the web development to turn it into a web application.&lt;/p&gt;

&lt;p&gt;Through interacting with the tool audiences are able to learn when a face is no longer a face in the eyes of facial recognition software and contribute to an alternative facial database of adverserial self-portraits.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/pyfa/shinji1.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/pyfa/shinji2.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="artist-res" /><category term="ML" /><category term="web" /><summary type="html">Over the summer Alejandro lent a hand to help develop Shinji Toya latest interactive art project, Paint Your Face Away.</summary></entry><entry><title type="html">Christopher Macinnes - Artist Residency Program 2020</title><link href="/artist-res/2020/08/04/Chris-Macinnes.html" rel="alternate" type="text/html" title="Christopher Macinnes - Artist Residency Program 2020" /><published>2020-08-04T13:00:00+00:00</published><updated>2020-08-04T13:00:00+00:00</updated><id>/artist-res/2020/08/04/Chris-Macinnes</id><content type="html" xml:base="/artist-res/2020/08/04/Chris-Macinnes.html">&lt;p&gt;&lt;img src=&quot;/img/res01/rpg2.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We are very pleased to announce &lt;a href=&quot;http://Christophertophermacinnes.com/&quot;&gt;Christopher Macinnes&lt;/a&gt; as the first artist to take part in our (online) residency program for 2020. Christopher was selected through our collaboration with &lt;a href=&quot;http://www.offsiteproject.org/&quot;&gt;Offsite Projects&lt;/a&gt; where he presented &lt;a href=&quot;http://www.offsiteproject.org/ZIP&quot;&gt;Imagine escape in darkness&lt;/a&gt; which you can dowload and view offline as part of their ZIP residency program.&lt;/p&gt;

&lt;p&gt;Christopher MacInnes is an artist based in London. Taking computing and networks as a starting point he works with software, hardware and occasionally organisms. Using the diverse vectors of our planetary networks, MacInnes attempts to trace the mycellenic tangle of chaotic phenomena across platforms, landscapes and bio-synthetic ecologies.&lt;/p&gt;

&lt;p&gt;We have begun working with Christopher to help develop and realise a multiplayer online game in which the audience will explore a an imagined world full of mystical artefacts and aspects of techno-spirtuality and gnosticism. The game is already looking really exciting, see some of the images below and Christopher has been developing the environment using three.js, websockets and node and we will hopefully be sharing some technical guides to accompany the new work for those interested very soon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res01/rpg1.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="artist-res" /><category term="web" /><summary type="html">We are very pleased to announce Christopher Macinnes as the first artist to take part in our (online) residency program for 2020!</summary></entry><entry><title type="html">Residency Reflections: Tamara Kametani</title><link href="/artist-res/2019/12/10/tamara-kametani-google-car.html" rel="alternate" type="text/html" title="Residency Reflections: Tamara Kametani" /><published>2019-12-10T13:00:00+00:00</published><updated>2019-12-10T13:00:00+00:00</updated><id>/artist-res/2019/12/10/tamara-kametani-google-car</id><content type="html" xml:base="/artist-res/2019/12/10/tamara-kametani-google-car.html">&lt;h3 id=&quot;chasing-the-google-maps-car-by-alejandro-ball&quot;&gt;Chasing the Google Map’s Car by Alejandro Ball&lt;/h3&gt;

&lt;h2 id=&quot;artist-residency&quot;&gt;Artist Residency&lt;/h2&gt;

&lt;h3 id=&quot;the-beginning&quot;&gt;The beginning&lt;/h3&gt;
&lt;p&gt;Tamara had a very clear concept and roadmap for the project she wanted to undertake during her residency time with Agorama. The concept… locate Google Street View recording car and secretly insert herself into Google Map.&lt;/p&gt;

&lt;p&gt;The idea originated from an earlier moment when Tamare was still attending her MA with the Royal Academy of Art (London, UK), where she described a moment when she was walking to class and suddenly realised the famous Google Street View car had passed her. In her curiousity, Tamara explored Google Maps in the location of her interaction to discover that she had in fact been captured serindipitously and now was a permenant main stay of the world’s largest digital interactive map.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect2/tamara-beginning.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It was with this experience in mind that Tamara approach the Agorama residency; an attempt to reenact this previous experience through the residency process.&lt;/p&gt;

&lt;h3 id=&quot;residencys-approach&quot;&gt;Residency’s approach&lt;/h3&gt;

&lt;p&gt;As for the approach to Tamara’s residency, this brough Agorama into a position of researcher and proposing various tools and platforms that could possible help Tamara achieve her vision.&lt;/p&gt;

&lt;p&gt;In our first initial meeting we discussed several options for tracking her progress in searching out the Google Street View car’s route, from using GPS tracking devices to platforms where she could document the progress of her project. In the end Tamara settled on using Google’s own &lt;a href=&quot;https://mymaps.google.com&quot;&gt;My Maps&lt;/a&gt;, which allows for a user to create their own Google map with landmarks, icon placement and the highlighing of walking and driving routes.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect2/Kametani_T_01.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We also pursued other options in the case of Tamara failing to find he Google Street View car through the means of other potential surveillance material. One particular example which was attempted and not used for the final work of art is the &lt;a href=&quot;https://api.tfl.gov.uk/Place/Type/JamCam&quot;&gt;TFL Jam Cam public API database&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The final elements, which were enacted for Tamara in her project’s endeavour were the create’s of a closed Whatsapp group of trusted individuals and a meeting with one of the Google Street View car’s managers. In the former case, participants that were invited to the closed Whatsapp group were instructed to keep an eye out for the Google Street View car and to photograph it then share it with the group along with the location of the sighting.&lt;/p&gt;

&lt;p&gt;In the latter action, Agorama were able - through insider connections ;) - to score a meeting with a Google Street View car manager. Due to this sensitive nature this is all that will be said on the matter.&lt;/p&gt;

&lt;h2 id=&quot;processes-overview&quot;&gt;Processes overview&lt;/h2&gt;

&lt;h3 id=&quot;tfl-jam-cam-api&quot;&gt;TFL Jam Cam API&lt;/h3&gt;
&lt;p&gt;While not included into the final project, this aspect of the collaboration was incredibly fun to put together for Tamara! Essentially it was agreed with Tamara that I would create a simple program that would repeatedly download cctv footage from TFL’s Jam Cam API within a 5 mile radius of Tamara’s Google Street View car search route.&lt;/p&gt;

&lt;p&gt;To provide context into the TFL API… the City of London is known as one of the worlds most surveilled cities in the world, with thousands of cameras scattered across the city. The interesting aspect in all this &lt;code class=&quot;highlighter-rouge&quot;&gt;Big Borther&lt;/code&gt; business is that these thousands of CCTV cameras are not under the control of a single party, and in the case of the CCTV camera attempted to be used for Tamara’s project TFL controls roughly 911 different CCTV &lt;code class=&quot;highlighter-rouge&quot;&gt;Jam Cams&lt;/code&gt; for traffic surveillance purposes. Thus, it was completely legally to work with this online data set (at least at the time)!&lt;/p&gt;

&lt;p&gt;Due to the way that footage is captured for the API, it was necessary to create a scrapping system that would repeatedly query the API data set. The reason for this was because TFL only uploads 5 second video clips of its CCTV footage every 5 mins. Thus, the approach, as seen below, was to sort through the data and discover which cameras were in the 5 mile radius that Tamara required. From there is the dataset matches our location criteria, we package the data we want into a json object. From this point the next step is to create a &lt;code class=&quot;highlighter-rouge&quot;&gt;fetch()&lt;/code&gt; request to the server we want to send the data files to.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;PLEASE NOTE: to attempt this your are required to have a working server that you control with this required storage space and API receiver system inside that server&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;At this point all we need to do is create this simple scrapping function into an interval function (meanign a function that runs periodically using the &lt;code class=&quot;highlighter-rouge&quot;&gt;setinterval()&lt;/code&gt; js function), and tada! You have a CCTV scrapping system that will bestow you with a huge data dump of Jam Cam footage.&lt;/p&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;        &lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;jamcall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;ev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
          &lt;span class=&quot;nx&quot;&gt;ev&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;preventDefault&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;

          &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;https://api.tfl.gov.uk/Place/Type/JamCam&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;51.4228&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;51.5967&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3891&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1531&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;modified&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

                &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
                  &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;media&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

                &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

                &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;http://target.API.receiver/&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;cors&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;omit&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Content-Type&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;application/json&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                  &lt;span class=&quot;na&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

              &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

          &lt;span class=&quot;nx&quot;&gt;setInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
            &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;https://api.tfl.gov.uk/Place/Type/JamCam&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
              &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
              &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;kd&quot;&gt;var&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;51.4228&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;51.5967&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3891&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lon&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1531&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;
                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;modified&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;));&lt;/span&gt;

                  &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;JSON&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;stringify&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;
                    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;media&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;myJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;additionalProperties&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;].&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;value&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;});&lt;/span&gt;

                  &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

                  &lt;span class=&quot;nx&quot;&gt;fetch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;http://target.API.receiver/&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;POST&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;cors&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;credentials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;omit&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;header&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;Content-Type&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;application/json&lt;/span&gt;&lt;span class=&quot;dl&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt;
                    &lt;span class=&quot;na&quot;&gt;body&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;camObj&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;response&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;json&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;();&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;then&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
                    &lt;span class=&quot;nx&quot;&gt;console&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;log&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;resJson&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;
                  &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

                &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
          &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;300000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;);&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;};&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If you would like to see that actual data dump of this process, please follow this link: &lt;a href=&quot;http://cctv-dump.agorama.org.uk/&quot;&gt;Agorama TfL datadump&lt;/a&gt;.
Currently, we only have on display the actual day that Tamara request this program to be used.&lt;/p&gt;

&lt;h3 id=&quot;closed-whatsapp-google-street-view-car-search-group-gallery&quot;&gt;Closed Whatsapp Google Street View car search group (Gallery)&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-1.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-3.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-5.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/img/res-reflect2/Whatsapp-grp-4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;tamara-kametanis-my-google-map&quot;&gt;Tamara Kametani’s My Google Map&lt;/h3&gt;

&lt;iframe src=&quot;https://www.google.com/maps/d/embed?mid=1rFaSi2vcBy80ss-csdHdD8Kx9hUYFsGO&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/iframe&gt;</content><author><name></name></author><category term="artist-res" /><category term="web" /><category term="code" /><summary type="html">Agorama's residency reflection focusing on Tamara Kametani's commission. Swayze Effect. By Alejandro Ball.</summary></entry><entry><title type="html">Residency Reflections: Hazel Brill</title><link href="/artist-res/2019/12/09/hazel-bill-ml-agents.html" rel="alternate" type="text/html" title="Residency Reflections: Hazel Brill" /><published>2019-12-09T13:00:00+00:00</published><updated>2019-12-09T13:00:00+00:00</updated><id>/artist-res/2019/12/09/hazel-bill-ml-agents</id><content type="html" xml:base="/artist-res/2019/12/09/hazel-bill-ml-agents.html">&lt;h1 id=&quot;machine-learning--unity-by-max-dovey&quot;&gt;Machine Learning &amp;amp; Unity by Max Dovey&lt;/h1&gt;

&lt;h2 id=&quot;artist-residency&quot;&gt;Artist Residency&lt;/h2&gt;

&lt;p&gt;Hazel Brill is an artist based in London who graduated from the Slade School of Art in 2017, where I first incidentally became captivated in her complacently titled ‘I made a show for you’ (2017). Hazel creates large scale diorama’s to host her videos, characters and narrations that are all projected onto 3d objects. She uses projection mapping to effectively create highly captivating 3d physical environments that the audience can inhabit without the need for any virtual reality goggles.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/outtosea0028-small.jpg&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As part of Hazel’s residency she had expressed an interest in how artificial intelligence can be used to automate or generate autonomous agents and characters. In one of our initial conversations we spent some time discussing how robotics, animatronics and neural networks relate to one another and indulging in speculating about how something like MIT’s AlterEgo project actually works (Spoiler – we have no idea). We looked at many ‘performances’ of artificial intelligent systems from animatronics to robotics and neural network interfaces - While fascinated by the detail on some of the latest animatronic models we also had to work within our resources of time and budget. Hazel often created characters or bodies that communicate through sounds in her work and we began to discuss how those bodies could be implanted with some sort of artificial intelligence to behave or act autonomously.&lt;/p&gt;

&lt;h2 id=&quot;technical-process&quot;&gt;Technical Process&lt;/h2&gt;

&lt;h3 id=&quot;aim--to-create-autonomous-animations-by-connecting-machine-learning-models-to-unity&quot;&gt;Aim – to create autonomous animations by connecting machine learning models to Unity&lt;/h3&gt;

&lt;h4 id=&quot;software-requirements&quot;&gt;software requirements&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;python 3.6&lt;/li&gt;
  &lt;li&gt;TensorFlow&lt;/li&gt;
  &lt;li&gt;Unity&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;useful-resources&quot;&gt;useful resources&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Unity-Technologies/ml-agents&quot;&gt;Unity Machine Learning Toolkit (beta)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-by-step-guide&quot;&gt;Step-by-step guide&lt;/h3&gt;

&lt;p&gt;Setting up your working environments&lt;/p&gt;

&lt;p&gt;1) Install unity according to your specific operating system (linux users are advised to use unityHub as it maintains the latest version)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/@indiecontessa/setting-up-a-python-environment-with-TensorFlow-on-macos-for-training-unity-ml-agents-faf19d71201&quot;&gt;guide for Mac OSX&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://medium.com/@tijmenlv/setting-up-ml-agents-on-linux-82972c353ad7&quot;&gt;guide for Ubuntu 16.04&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;2) Install the machine learning dependencies&lt;/p&gt;

&lt;p&gt;&lt;em&gt;We recommend using anaconda with python 3.6&lt;/em&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.cse.unsw.edu.au/~en1811/resources/getting-started/install-anaconda.html&quot;&gt;anaconda installation guide&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Alternatively, if you already have your own environment with python 3.6 installed you can skip installing an enconda environment and go straight to installing the dependencies for &lt;a href=&quot;&quot;&gt;ml-agents toolkit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;using pip3&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  install_requires=[
        &quot;mlagents_envs==0.8.2&quot;,
        &quot;tensorflow&amp;gt;=1.7,&amp;lt;1.8&quot;,
        &quot;Pillow&amp;gt;=4.2.1&quot;,
        &quot;matplotlib&quot;,
        &quot;numpy&amp;gt;=1.13.3,&amp;lt;=1.14.5&quot;,
        &quot;jupyter&quot;,
        &quot;pytest&amp;gt;=3.2.2,&amp;lt;4.0.0&quot;,
        &quot;docopt&quot;,
        &quot;pyyaml&quot;,
        &quot;protobuf&amp;gt;=3.6,&amp;lt;3.7&quot;,
        &quot;grpcio&amp;gt;=1.11.0,&amp;lt;1.12.0&quot;,
        'pypiwin32==223;platform_system==&quot;Windows&quot;',
    ],
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3) Virtual Environment&lt;/p&gt;

&lt;p&gt;use conda to create a virtual environment to run your ml-agents toolkit from (We created it with)&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda create --name tensor python=3.6 pip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is located in Documents/unity/ml-agents-master/&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda activate tensor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;to close we then type&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda deactivate
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) Install the ML-Agent toolkit&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ git clone https://github.com/Unity-Technologies/ml-agents.git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;training-models-with-unity--ml-agents&quot;&gt;Training Models with Unity &amp;amp; ML-Agents&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/ml-agents-workflow1.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;These terms won’t mean much until you start working but its good to introduce the ‘abstract’ concepts of how you are going to interface between TensorFlow &amp;gt; python &amp;gt; Unity.&lt;/p&gt;

&lt;p&gt;Three important things to remember:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;You need an academy to run the simulation&lt;/li&gt;
  &lt;li&gt;Within the the academy you need a brain object&lt;/li&gt;
  &lt;li&gt;Inside the brain you need to inject a (pre-trained) model&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/workflow2.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml-1.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;1-open-up-unity&quot;&gt;1) Open up Unity&lt;/h5&gt;

&lt;p&gt;Now you have successfully setup your environments you are ready to begin using the ml-agents toolkit with unity.&lt;/p&gt;

&lt;p&gt;Before training your own model its handy to examine the pre-fabs and demos that are available within the unity plugins manager within your unity program drag the complete Ml-agents folder into your assets folder within your project console this should import all the assets you need.&lt;/p&gt;

&lt;p&gt;Navigate to ML-agents in your Project Assets Window &amp;gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml-2.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;2-import-the-ml-agents-tutorials&quot;&gt;2) Import the ML-Agents Tutorials&lt;/h5&gt;

&lt;p&gt;For this tutorial, we are going to use the crawler example so navigate to&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;ML-agents &amp;gt; examples &amp;gt; Crawler &amp;gt; Scenes&amp;gt;
and launch the ‘Crawler Dynamic Scene’&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;this is going to generate a scene with multiple crawlers, you can go ahead and delete the 10 other crawlers for this tutorial as it will load faster&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml-2.0.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;3-give-your-spider-a-brain&quot;&gt;3) Give your spider a brain&lt;/h5&gt;

&lt;p&gt;In ML-agents &amp;gt; examples &amp;gt; Crawler &amp;gt; Brains&lt;/p&gt;

&lt;p&gt;select CrawlerDynamicLearning.asset and drag into your object panel (on the left)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml3.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;4-give-your-brain-a-pre-trained-model&quot;&gt;4) Give your brain a pre-trained model&lt;/h5&gt;

&lt;p&gt;This should now be visible and listed as ‘Academy’ layer, with the academy layer selected you need to give it a brain!&lt;/p&gt;

&lt;p&gt;Double click on the brain , this will bring up a new inspector window&lt;/p&gt;

&lt;p&gt;in ML-agents &amp;gt; examples &amp;gt; Crawler &amp;gt; TFModels&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-ml-4.png&quot; alt=&quot;alt-text&quot; /&gt;&lt;/p&gt;

&lt;p&gt;drag CrawlerDynamicLearning onto the brain&lt;/p&gt;

&lt;p&gt;this is a pretrained neural network model that has been trained to repeatedly follow the target (the yellow / orange box)&lt;/p&gt;

&lt;p&gt;once you have an academy layer, with a brain, with a pre-trained neural network in it you can hit the play button and watch the crawler in action.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/spider-crawl.gif&quot; style=&quot;display:block;margin-left:auto;margin-right:auto;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;going-further&quot;&gt;Going Further&lt;/h2&gt;

&lt;p&gt;Now you have figured out how to use the pre-trained models, you can then begin customising the TensorFlow models and reward functions to create your own models.&lt;/p&gt;

&lt;p&gt;Working within your virtual environment in your terminal window&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/res-reflect1/unity-mlagents.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Go to utlities open up ‘terminal’&lt;/li&gt;
  &lt;li&gt;Copy into terminal or type
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ cd  your/working/Directory/unity/ml-agents-master/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;From here you should activate your working environment
copy into terminal or type
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ conda activate tensor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Test that your ml-agents is installed by typing
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ mlagents-learn
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To train your own own model use this line below and hit the ‘play’ button when it asks….&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;$ mlagents-learn /Users/hazelbrill/Documents/unity/ml-agents-master/config/trainer_config.yaml --run-id=firstRun --train
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From here you should be able to see the training process in the unity editor window. To terminate the training press Ctrl C in the terminal window to stop the process.
the model should be saved and available to you within the Unity editor.
You can edit and change the training settings with the trainer_config.yaml&lt;/p&gt;

&lt;h5 id=&quot;after-training&quot;&gt;After training&lt;/h5&gt;
&lt;p&gt;You can press Ctrl+C to stop the training, and your trained model will be at &lt;code class=&quot;highlighter-rouge&quot;&gt;models/&amp;lt;run-identifier&amp;gt;/&amp;lt;brain_name&amp;gt;.nn&lt;/code&gt; where &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;brain_name&amp;gt;&lt;/code&gt; is the name of the Brain corresponding to the model. (Note: There is a known bug on Windows that causes the saving of the model to fail when you early terminate the training, it’s recommended to wait until Step has reached the max_steps parameter you set in trainer_config.yaml.) This file corresponds to your model’s latest checkpoint. You can now embed this trained model into your Learning Brain by following the steps below, which is similar to the steps described above.&lt;/p&gt;

&lt;h5 id=&quot;helpful-resources&quot;&gt;helpful resources&lt;/h5&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md&quot;&gt;https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Basic-Guide.md&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://gym.openai.com/&quot;&gt;https://gym.openai.com/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=FXFrwvV60N4&amp;amp;list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&amp;amp;index=3&quot;&gt;https://www.youtube.com/watch?v=FXFrwvV60N4&amp;amp;list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&amp;amp;index=3&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Unity ML tutorial - &lt;a href=&quot;https://www.youtube.com/watch?v=x2RBxmooh8w&quot;&gt;https://www.youtube.com/watch?v=x2RBxmooh8w&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Another series - &lt;a href=&quot;https://www.youtube.com/watch?v=uiutRBXfEbg&amp;amp;list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&amp;amp;index=2&quot;&gt;https://www.youtube.com/watch?v=uiutRBXfEbg&amp;amp;list=PLX2vGYjWbI0R08eWQkO7nQkGiicHAX7IX&amp;amp;index=2&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=x2RBxmooh8w&quot;&gt;https://www.youtube.com/watch?v=x2RBxmooh8w&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/?_ga=2.221967515.2097880843.1563531955-2130937146.1561465028&quot;&gt;https://blogs.unity3d.com/2018/12/17/ml-agents-toolkit-v0-6-improved-usability-of-brains-and-imitation-learning/?_ga=2.221967515.2097880843.1563531955-2130937146.1561465028&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/&quot;&gt;https://blogs.unity3d.com/2017/12/11/using-machine-learning-agents-in-a-real-game-a-beginners-guide/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="artist-res" /><category term="ML" /><category term="code" /><summary type="html">Agorama's residency reflection focusing on Hazel Brill's commission. Swayze Effect. By Max Dovey.</summary></entry><entry><title type="html">Swayze effect</title><link href="/projects/2019/11/01/swayze-effect.html" rel="alternate" type="text/html" title="Swayze effect" /><published>2019-11-01T11:00:00+00:00</published><updated>2019-11-01T11:00:00+00:00</updated><id>/projects/2019/11/01/swayze-effect</id><content type="html" xml:base="/projects/2019/11/01/swayze-effect.html">&lt;p&gt;&lt;img src=&quot;/img/projects/swayze/swayze-4.jpg&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;Swayze effect&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Hazel Brill, James Irwin and Tamara Kametani&lt;/p&gt;

&lt;p&gt;Curated by Agorama at &lt;a href=&quot;http://www.platformsouthwark.co.uk/&quot;&gt;Platform Southwark&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;12–21 September 2019&lt;/p&gt;

&lt;p&gt;&lt;em&gt;swayze:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;The sensation of having no tangible relationship with your surroundings despite feeling embodied in the virtual world1&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Swayze effect presents the works produced as a result of Agorama’s Artistic Residencies &amp;amp; Exhibitions Programme. Hazel Brill, James Irwin and Tamara Kametani were invited to use Agorama’s studio at Raven Row from April 2019 to conceive new work informed by technology. Through collaboration and computer programming, this residency aimed to provide the artists with access to digital tools for the development of their practices.&lt;/p&gt;

&lt;p&gt;Using emerging technologies such as augmented reality, machine learning and digital mapping, the artists trace, transform and track bodily movements from the physical to the digital. The term ‘Swayze effect’ is used in virtual reality design to describe the struggle of affecting virtual or digital environments with embodied feedback and physical presence. All the works in the exhibition suggest an intervention into an interface – possible ways of navigating and experiencing the world around us through augmented viewfinders. &lt;/p&gt;

&lt;p&gt;Hazel Brill used her residency to experiment with animating characters, applying machine learning and game engines to generate locomotion processes such as climbing and crawling. She invites the viewer to step into a rendered plateau inhabited only by biomorphic organisms that move, interact and grow through reinforcement learning algorithms. Tamara Kametani has been attempting to hunt down a Google Street View car, inspired by a previous accidental encounter with it in 2017. In an effort to assert some form of control over her virtual depiction, the artist used her body as a disruptive tool, devising an on-going game of deception with Google in order to ‘hack’ its global scope in the most analogue way possible – physical human interference. James Irwin makes use of augmented reality to delineate the space between bodies in physical spaces and their digital facsimiles, questioning the fragmentation of identity in the virtual realm. Throughout his residency, Irwin used video to document his reflection and manipulated those recordings through various filter degradation effects, simulating an augmented reality environment to encapsulate digital loss. By inviting the viewer to experience his robotic mirror sculpture, Irwin aims to trap one’s gaze in this reflective &lt;em&gt;mise en abyme&lt;/em&gt; – a corroded landscape.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Burdette, M. &lt;a href=&quot;https://storystudio.oculus.com/en-us/blog/the-swayze-effect/&quot;&gt;‘The Swayze Effect’&lt;/a&gt; Story Studio Blog Website.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&quot;http://www.hazelbrill.com/&quot;&gt;Hazel Brill&lt;/a&gt; works across video, animation, sound, sculpture, text and performance. She is interested in creating ‘shows’ that merge disparate contexts and digressive narratives, playing with storytelling in popular culture.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.tamarakametani.com/&quot;&gt;Tamara Kametani&lt;/a&gt; is an artist working across a variety of media including installation, video, photography and sculpture with an emphasis on site-specificity. Amongst the underlying concerns in her practice are the topics surrounding power relations, surveillance, privacy, and access to information. She is particularly interested in the role that technology plays in the construction of contemporary and historical narratives and the new experiences it enables. The complex relationship between aesthetics and politics is at the core of the inquiry in Kametani’s practice. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.jamesirwin.net&quot;&gt;James Irwin&lt;/a&gt; uses digital media, printmaking, sculpture, sound and moving image to rework and manipulate content from on and offline sources to shift or skew the relationship between the physical world and its digital image. His work investigates the capacity of physical and digital media to evoke/provoke authentic experience in a post truth context where anxiety and uncertainty become valid or unavoidable creative positions.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/swayze/swayze-1.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/img/projects/swayze/swayze-2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;img src=&quot;/img/projects/swayze/swayze-3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><category term="exhibitions" /><summary type="html">Swayze Effect presents the works produced as a result of Agorama’s 2019 Artistic Residencies Programme. The term ‘Swayze effect’ is used in virtual reality design to describe the struggle of affecting virtual or digital environments with embodied feedback and physical presence. Exhibiting artists - Hazel Brill, James Irwin and Tamara Kametani. Exhibition hosted by Platform Southwark.</summary></entry><entry><title type="html">The Server Co-op</title><link href="/projects/2019/06/30/coop-project-overview.html" rel="alternate" type="text/html" title="The Server Co-op" /><published>2019-06-30T11:00:00+00:00</published><updated>2019-06-30T11:00:00+00:00</updated><id>/projects/2019/06/30/coop-project-overview</id><content type="html" xml:base="/projects/2019/06/30/coop-project-overview.html">&lt;p&gt;&lt;img src=&quot;/img/projects/co-op/coop-1.jpg&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;We organised public workshops at Mozilla festival, free university and some european universitys and colleges. The research manifested in a number of creative applications and interpretation of p2p internet culture and some documented code for turning raspberry pi’s into p2p web servers that can be found on our github page and the rest of the research resides here.&lt;/p&gt;

&lt;p&gt;We host regular p2p web meet-ups at our studio at Raven Row for anyone interested in the distributed internet. The aim is to facilitate a community interested in experimenting, developing and maintaining a distributed internet infrastructure. We have been invited to give workshops and lead regular educational programs exploring the potential for a self organized distributed hosting platform for Agorama.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/co-op/coop2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Agorama established and hosted a working research group into the distributed p2p (peer-to-peer) web at their studio at Raven Row between Sep. 2018 - Jun. 2019. The aim was to foster a community interested in experimenting, developing and maintaining a distributed Internet infrastructure.</summary></entry><entry><title type="html">Moses the Lonely Londoner</title><link href="/projects/2019/05/30/moses.html" rel="alternate" type="text/html" title="Moses the Lonely Londoner" /><published>2019-05-30T11:00:00+00:00</published><updated>2019-05-30T11:00:00+00:00</updated><id>/projects/2019/05/30/moses</id><content type="html" xml:base="/projects/2019/05/30/moses.html">&lt;p&gt;&lt;img src=&quot;/img/projects/moses/moses-4.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Moses the Lonely Londoner is a project set on investigating the concept of intimate digital spaces, namely that of the telecocoon which was first detailed in Japanese teen keitai culture as an invisible space of intimacy linked together by an individual’s mobile device.&lt;/p&gt;

&lt;p&gt;The project takes inspiration from author Sam Selvon’s novel The Lonely Londoners and makes manifest its main character, Moses Aloetta, through the use of Artificial Intelligence. Drawing parallels between the loneliness expressed in Selvon’s 1950s London and the notions of isolation propelled by Social Media, users are invited to connect with Moses on their mobile device and enter his telecocoon.&lt;/p&gt;

&lt;p&gt;This project was exhibited in the Photographers’ Gallery as part of the exhibition “For the Time Being”, and was commissioned by the Royal College of Art in partnership with the Photographers’ Gallery.&lt;/p&gt;

&lt;p&gt;For the Time Being was a group exhibition curated by six postgraduate students seeking to explore the shifting responsibilities of institutions in this networked age, as part of the MA Curating Contemporary Art Programme Graduate Projects 2019 at Royal College of Art, London. The exhibition was produced in collaboration with The Photographers’ Gallery in May 2019.&lt;/p&gt;

&lt;p&gt;Links…&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/moses/moses-3.png&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/moses/moses-1.jpg&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/img/projects/moses/moses-2.jpg&quot; alt=&quot;&quot; /&gt;
&lt;br /&gt;&lt;/p&gt;</content><author><name></name></author><category term="projects" /><summary type="html">Moses the Lonely Londoner is a project set on investigating the concept of intimate digital spaces linked together by an individual's mobile device. This project was exhibited in the Photographers' Gallery as part of the exhibition &quot;For the Time Being&quot;, and was commissioned by the Royal College of Art in partnership with the Photographers' Gallery.</summary></entry></feed>